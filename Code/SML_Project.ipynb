{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import os\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "# Downloading necessary NLTK resources\n",
    "nltk.download('punkt')       # Tokenizer Model\n",
    "nltk.download('stopwords')   # Stopwords\n",
    "nltk.download('wordnet')     # Lemmatizer\n",
    "\n",
    "# Get the current working directory\n",
    "script_dir = os.getcwd()\n",
    "\n",
    "# Loading the CSV files from the current working directory\n",
    "file_path1 = os.path.join(script_dir, 'True.csv')\n",
    "file_path2 = os.path.join(script_dir, 'Fake.csv')\n",
    "\n",
    "# Loading the CSV files with specified data types\n",
    "data1 = pd.read_csv(file_path1, dtype=str)\n",
    "data2 = pd.read_csv(file_path2, dtype=str)\n",
    "\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "pd.set_option('display.max_rows', 2)\n",
    "#print(data1.head(1))\n",
    "#print(data2.head(1))\n",
    "\n",
    "# Concatenate the two dataframes\n",
    "data = pd.concat([data1, data2], ignore_index=True)\n",
    "output_file_path = os.path.join(script_dir, 'News_Dataset.csv')\n",
    "data.to_csv(output_file_path, index=False)\n",
    "#print(data1.columns)\n",
    "#print(data2.columns)\n",
    "# Specify the names of the five columns\n",
    "#specified_columns = ['title', 'text', 'subject', 'date', 'type']\n",
    "\n",
    "# Filter the DataFrame to include rows where all specified columns have data\n",
    "#filtered_data = data.dropna(subset=specified_columns, how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize NLTK's WordNet lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Cleans and preprocesses the input text.\n",
    "    \n",
    "    Parameters:\n",
    "    text (str): The text to preprocess.\n",
    "    \n",
    "    Returns:\n",
    "    str: The cleaned and preprocessed text.\n",
    "    \"\"\"\n",
    "    # Tokenize the text into words\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Convert tokens to lowercase\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "\n",
    "    # Remove stopwords (common words that do not carry much meaning)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    # Remove punctuation and special characters\n",
    "    tokens = [token for token in tokens if token not in string.punctuation]\n",
    "\n",
    "    # Lemmatize tokens to their base form\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    # Remove tokens that are not alphabetic\n",
    "    tokens = [token for token in tokens if token.isalpha()]\n",
    "\n",
    "    # Join tokens back into a single string\n",
    "    clean_text = ' '.join(tokens)\n",
    "\n",
    "    return clean_text\n",
    "\n",
    "data['Tokenized Text'] = data['text'].apply(preprocess_text)\n",
    "# pd.set_option('display.max_colwidth', None)\n",
    "# pd.set_option('display.max_rows', 2)\n",
    "print(data['Tokenized Text'].head(1))\n",
    "\n",
    "# Define the output file path for the tokenized data\n",
    "tokenized_output_file_path = tokenized_data.csv\n",
    "\n",
    "# Save the tokenized data to a CSV file\n",
    "data.to_csv(tokenized_output_file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training (70%) and test (30%) sets\n",
    "train_data, test_data = train_test_split(data, test_size=0.3, random_state=42)\n",
    "\n",
    "# Define the output file paths for the split datasets\n",
    "train_output_file_path = train_data.csv\n",
    "test_output_file_path = test_data.csv\n",
    "\n",
    "# Save the split datasets to CSV files\n",
    "train_data.to_csv(train_output_file_path, index=False)\n",
    "test_data.to_csv(test_output_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training (70%) and test (30%) sets\n",
    "train_data, test_data = train_test_split(data, test_size=0.3, random_state=42)\n",
    "\n",
    "# Define the output file paths for the split datasets\n",
    "train_output_file_path = train_data.csv\n",
    "test_output_file_path = test_data.csv\n",
    "\n",
    "# Save the split datasets to CSV files\n",
    "train_data.to_csv(train_output_file_path, index=False)\n",
    "test_data.to_csv(test_output_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, log_loss\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the data\n",
    "train_data = pd.read_csv('train_data.csv')\n",
    "test_data = pd.read_csv('test_data.csv')\n",
    "\n",
    "# Replace missing values with empty strings\n",
    "train_data['Tokenized Text'].fillna('', inplace=True)\n",
    "test_data['Tokenized Text'].fillna('', inplace=True)\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X_train, y_train = train_data['Tokenized Text'], train_data['type']\n",
    "X_test, y_test = test_data['Tokenized Text'], test_data['type']\n",
    "\n",
    "# Define models\n",
    "models = [\n",
    "    ('Naive Bayes', make_pipeline(TfidfVectorizer(), MultinomialNB())),\n",
    "    ('Logistic Regression', make_pipeline(TfidfVectorizer(), LogisticRegression())),\n",
    "    ('Random Forest', make_pipeline(TfidfVectorizer(), RandomForestClassifier())),\n",
    "    ('Gradient Boosting', make_pipeline(TfidfVectorizer(), GradientBoostingClassifier())),\n",
    "    #('Support Vector Machine', make_pipeline(TfidfVectorizer(), SVC(probability=True)))\n",
    "]\n",
    "\n",
    "# Train models and evaluate\n",
    "results = {'Model': [], 'Test Accuracy': [], 'Test Precision': [], 'Test F1': [], 'Test Loss': []}\n",
    "\n",
    "for name, model in models:\n",
    "    print(\"Training\", name, \"model...\")\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Test set evaluation\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    test_precision = precision_score(y_test, y_test_pred, average='weighted')\n",
    "    test_f1 = f1_score(y_test, y_test_pred, average='weighted')\n",
    "    test_loss = log_loss(y_test, model.predict_proba(X_test))\n",
    "    \n",
    "    # Save results\n",
    "    results['Model'].append(name)\n",
    "    results['Test Accuracy'].append(test_accuracy)\n",
    "    results['Test Precision'].append(test_precision)\n",
    "    results['Test F1'].append(test_f1)\n",
    "    results['Test Loss'].append(test_loss)\n",
    "    print(name, \"model trained and evaluated successfully.\")\n",
    "\n",
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Plot accuracy, precision, and F1 scores\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(results_df['Model'], results_df['Test Accuracy'], color='skyblue')\n",
    "plt.title('Test Accuracy Comparison')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Model')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(results_df['Model'], results_df['Test Loss'], color='salmon')\n",
    "plt.title('Test Log Loss Comparison')\n",
    "plt.ylabel('Log Loss')\n",
    "plt.xlabel('Model')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Load the data\n",
    "train_data = pd.read_csv('train_data.csv')\n",
    "test_data = pd.read_csv('test_data.csv')\n",
    "\n",
    "# Handle missing values by replacing NaN with an empty string\n",
    "train_data['Tokenized Text'].fillna('', inplace=True)\n",
    "test_data['Tokenized Text'].fillna('', inplace=True)\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X_train, y_train = train_data['Tokenized Text'], train_data['type']\n",
    "X_test, y_test = test_data['Tokenized Text'], test_data['type']\n",
    "\n",
    "# Define the TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Transform text data into TF-IDF features\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Define and train the MLP Classifier\n",
    "mlp_classifier = MLPClassifier(hidden_layer_sizes=(100,))\n",
    "mlp_classifier.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Evaluate the MLP Classifier\n",
    "y_test_pred = mlp_classifier.predict(X_test_tfidf)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(\"Testing Accuracy (Neural Network):\", test_accuracy)\n",
    "\n",
    "# Visualize the results\n",
    "results_df = pd.DataFrame({\n",
    "    'Model': ['Neural Network'],\n",
    "    'Testing Accuracy': [test_accuracy]\n",
    "})\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.barplot(x='Model', y='Testing Accuracy', data=results_df, color='orange')\n",
    "plt.title('Model Accuracy Comparison')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Model')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the data\n",
    "train_data = pd.read_csv('train_data.csv')\n",
    "test_data = pd.read_csv('test_data.csv')\n",
    "\n",
    "# Replace missing values with empty strings\n",
    "train_data['Tokenized Text'].fillna('', inplace=True)\n",
    "test_data['Tokenized Text'].fillna('', inplace=True)\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X_train, y_train = train_data['Tokenized Text'], train_data['type']\n",
    "X_test, y_test = test_data['Tokenized Text'], test_data['type']\n",
    "\n",
    "# Define models\n",
    "models = [\n",
    "    ('Naive Bayes', make_pipeline(TfidfVectorizer(), MultinomialNB())),\n",
    "    ('Logistic Regression', make_pipeline(TfidfVectorizer(), LogisticRegression())),\n",
    "    ('Random Forest', make_pipeline(TfidfVectorizer(), RandomForestClassifier())),\n",
    "    ('Gradient Boosting', make_pipeline(TfidfVectorizer(), GradientBoostingClassifier()))\n",
    "]\n",
    "\n",
    "# Train models and evaluate\n",
    "results = {'Model': [], 'Test Accuracy': [], 'Test Precision': [], 'Test Recall': [], 'Test F1': []}\n",
    "\n",
    "for name, model in models:\n",
    "    print(\"Training\", name, \"model...\")\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Test set evaluation\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    test_precision = precision_score(y_test, y_test_pred, average='weighted')\n",
    "    test_recall = recall_score(y_test, y_test_pred, average='weighted')\n",
    "    test_f1 = f1_score(y_test, y_test_pred, average='weighted')\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    confusion_mat = confusion_matrix(y_test, y_test_pred)\n",
    "    \n",
    "    # Save results\n",
    "    results['Model'].append(name)\n",
    "    results['Test Accuracy'].append(test_accuracy)\n",
    "    results['Test Precision'].append(test_precision)\n",
    "    results['Test Recall'].append(test_recall)\n",
    "    results['Test F1'].append(test_f1)\n",
    "    print(name, \"model trained and evaluated successfully.\")\n",
    "\n",
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Print results\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the datasets\n",
    "train_data = pd.read_csv(train_output_file_path)\n",
    "test_data = pd.read_csv(test_output_file_path)\n",
    "\n",
    "# Encode the target column\n",
    "label_encoder = LabelEncoder()\n",
    "train_data['type'] = label_encoder.fit_transform(train_data['type'])\n",
    "test_data['type'] = label_encoder.transform(test_data['type'])\n",
    "\n",
    "# Fill missing values in the text data\n",
    "train_data['Tokenized Text'].fillna('', inplace=True)\n",
    "test_data['Tokenized Text'].fillna('', inplace=True)\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "X_train = train_data['Tokenized Text']\n",
    "y_train = train_data['type']\n",
    "X_test = test_data['Tokenized Text']\n",
    "y_test = test_data['type']\n",
    "\n",
    "# Initialize TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the text data into TF-IDF vectors for training and testing sets\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Train SVM model with Stochastic Gradient Descent\n",
    "svm_model = SGDClassifier(loss='hinge', penalty='l2', alpha=0.0001, random_state=42, max_iter=1000, tol=1e-3)\n",
    "\n",
    "# Collect loss values during training for both training and testing datasets\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "n_iterations = 1000  # Number of iterations for training\n",
    "\n",
    "for i in range(n_iterations):\n",
    "    svm_model.partial_fit(X_train_tfidf, y_train, classes=np.unique(y_train))\n",
    "    train_loss = np.mean(np.maximum(0, 1 - y_train * svm_model.decision_function(X_train_tfidf)))\n",
    "    test_loss = np.mean(np.maximum(0, 1 - y_test * svm_model.decision_function(X_test_tfidf)))\n",
    "    train_losses.append(train_loss)\n",
    "    test_losses.append(test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss function versus iterations for both training and testing datasets\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(np.arange(n_iterations), train_losses, label='Training Data')\n",
    "plt.plot(np.arange(n_iterations), test_losses, label='Testing Data')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Hinge Loss')\n",
    "plt.title('Convergence of Hinge Loss Function')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Calculate accuracy on the test set\n",
    "y_pred = svm_model.predict(X_test_tfidf)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy on the test set:\", accuracy)\n",
    " \n",
    "#Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Calculate precision, recall, and F1 score\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xgboost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the datasets\n",
    "train_data = pd.read_csv(train_data.csv\")\n",
    "test_data = pd.read_csv(test_data.csv\")\n",
    "\n",
    "# Encode the target column\n",
    "label_encoder = LabelEncoder()\n",
    "train_data['type'] = label_encoder.fit_transform(train_data['type'])\n",
    "test_data['type'] = label_encoder.transform(test_data['type'])\n",
    "\n",
    "# Fill missing values in the text data\n",
    "train_data['Tokenized Text'].fillna('', inplace=True)\n",
    "test_data['Tokenized Text'].fillna('', inplace=True)\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "X_train = train_data['Tokenized Text']\n",
    "y_train = train_data['type']\n",
    "X_test = test_data['Tokenized Text']\n",
    "y_test = test_data['type']\n",
    "\n",
    "# Initialize TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the text data into TF-IDF vectors for training and testing sets\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Initialize XGBoost classifier\n",
    "clf = xgb.XGBClassifier(\n",
    "    objective='binary:logistic',  # Make sure this matches your type of classification\n",
    "    learning_rate=0.1,\n",
    "    n_estimators=100,\n",
    "    max_depth=6,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Train XGBoost model\n",
    "clf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predictions\n",
    "predictions = clf.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Accuracy on the test set: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion matrix for XGboost\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, predictions)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "plot_confusion_matrix(clf, X_test_tfidf, y_test, cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize empty lists to store accuracy values\n",
    "train_accuracy = []\n",
    "test_accuracy = []\n",
    "\n",
    "# Train XGBoost model with evaluation metrics\n",
    "clf.fit(X_train_tfidf, y_train, eval_metric=['error'], eval_set=[(X_train_tfidf, y_train), (X_test_tfidf, y_test)], verbose=False)\n",
    "\n",
    "# Record accuracy for each boosting round\n",
    "eval_results = clf.evals_result()\n",
    "train_accuracy = 1 - np.array(eval_results['validation_0']['error'])\n",
    "test_accuracy = 1 - np.array(eval_results['validation_1']['error'])\n",
    "\n",
    "# Plot accuracy over boosting rounds\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_accuracy, label='Training Accuracy')\n",
    "plt.plot(test_accuracy, label='Testing Accuracy')\n",
    "plt.xlabel('Boosting Round')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy Over Boosting Rounds')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
